# TP-Transformer
The main novelty introduced from this project is the Tp-attention mechanism, a new Self-Attention mechanism, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention.
